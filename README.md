# Data_engineer_projects

Here I added 9 projects which have been made by me during my apprenticeship in Yandex.Practicum. Link for the platform https://practicum.yandex.ru/data-engineer/

## Table of content:

|**Project**| **Topic**|**Task**|**Tools**|**Results**|**project keywords**|
|----------|----------|-------|--------|--------|---|
|**[9. Final]()**|Batch processing|Create a pipeline, which retrieves data from Postgres for the given time period and uploads data extract to the Vertica|Postgres, Airflow, Vertica, Python, Metabase| Stable pipeline has been developed |Batch Processing; Airflow Connections; DAGs;|
|**[98. Creating DWH using cloud technologies for a food delivery aggregator.]()**|Cloud services |Receive real-time data from the Kafka broker, process and decompose into different layers of the data warehouse|Kafka, PostgreSQL, Redis, kubernetes, Python, Docker, Yandex Cloud|Using Yandex Cloud tools, three data processing microservices have been created, which were used to build and populate a data warehouse (DWH) for a food delivery aggregator.|Stream Processing; Microservice Architecture; Cloud Technologies |
**[7. Setting up data streaming for a food delivery aggregator.]()** |Real time data processing|Receive messages from Kafka, process and send to two receivers: a Postgres database and a new topic for the Kafka broker|PySpark, Kafka, PostgreSQL, Spark Streaming, Python |The data streaming service has been created that allows businesses to test a new development — a subscription to restaurants, through which subscribers will receive exclusive deals on restaurant dishes.|Stream Processing; Apache Spark Structured Streaming; Apache Kafka; Consumer; Producer; Stream-Static Join|
**[6. Updating the data warehouse for a social network.]()** |Data Lake|Create data marts on regular basis in Apache Hadoop file system|Hadoop, MapReduce, HDFS, Apache Spark |The Data Lake structure for the social network has been updated, and new data marts have been added that will use information based on user action coordinates.|Apache Spark; Data Lake; Big Data; Batch Processing; Geospatial Data|
**[5. Finding communities with a high conversion rate on the first message.]()** |Analytical databases|Build an analytical storage based on Vertica using Data Vault storage model. |Python, PostgreSQL, Vertica, Airflow, S3, REST-API |Created analytical datawarehouse with 2 layares based on Verica DB|Analytical DBMS; Distributed Data Processing (DDP); Data Models; Data Vault|
**[4. Implementation of a data mart for calculating payments to couriers.]()** |ETL pipepline creation|Study the API of the order delivery system. Design the table structure for the layers in the data warehouse. Implement the DAG. |Python, PostgreSQL, MongoDB, Airflow, REST-API |Courier payment amounts have been calculated: a DWH and ETL pipeline for the data have been implemented.|Data Layers; Data Marts; Slowly Changing Dimension (SCD); Incremental Loading|
**[3. Updated data processing pipeline]()** | ETL pipepline creation|Change existing pipeline considering modifications in DB |Python, PostgreSQL, Airflow, S3, REST-API | The created data processing pipeline for the online store has been updated: data on order cancellations and refunds have been added to the data mart, and metrics for "customer retention" have been calculated. | ETL и ELT; Airflow Connections; DAGs; ETL-Python; Batch Processing|
**[2. Optimization of the data model for the online store.]()** |optimize DB structure |Distribute data from a single table into multiple logical tables and create a user-friendly data view based on the new logical tables. |Python, PostgreSQL |The load on the online store's data warehouse has been optimized: data has been migrated from a large, unwieldy table into separate logical tables, and a data mart has been built based on these tables.|Data Layers; Dimensions; Facts; Views|
**[1. Creating a data mart for RFM (Recency, Frequency, Monetary) classification of users for a food delivery aggregator.]()** |datamarts building |Create RFM segmentation in local DB |SQL, PostgreSQL |Scripts for user segmentation have been created|Data Quality; Data Marts; SQL|